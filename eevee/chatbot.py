import os
import pathlib
import traceback
from datetime import datetime
from typing import Set, Dict, List, Generator, Any, Tuple
from .messages import Messages, Message, ToolCall
from .tools import tools_params_definitions
from .color_logger import get_logger
from .framework_models import get_model_framework
from .saved_chat import SavedChat
from .chat_connectors.connector_interface import Connector
from .chat_connectors.openai_connector import OpenAIConnector
from .chat_connectors.anthropic_connector import AnthropicConnector
from .chat_connectors.mistral_connector import MistralConnector
from ._types import Framework


class Chatbot:
    METADATA_TOKEN = "$%^"

    def __init__(self, available_frameworks: Set[Framework]) -> None:
        if not available_frameworks:
            raise RuntimeError('No available frameworks found! Make sure you supplied API keys')

        self.clients: Dict[Framework, Connector] = dict()
        self.callables = {f.__name__: f for f in tools_params_definitions.keys()}
        self.tools = self._build_tools()
        self.messages = Messages()
        self.logger = get_logger()
        self.start_time = datetime.now()

        for framework in available_frameworks:
            match framework:
                case 'openai':
                    client = OpenAIConnector()
                case 'mistral':
                    client = MistralConnector()
                case 'anthropic':
                    client = AnthropicConnector()
            self.clients[framework] = client

        self.INFO_TOKEN = self.clients[list(self.clients.keys())[0]].INFO_TOKEN
        self.WARNING_TOKEN = self.clients[list(self.clients.keys())[0]].WARNING_TOKEN

    def _build_tools(self) -> List[Dict[str, Any]]:
        tools = list()
        for func, v in tools_params_definitions.items():
            params = {}
            required = []
            for p in v:
                params[p[0]] = p[1]
                if p[2]: 
                    required.append(p[0])

            tools.append({
                "type": "function",
                "function": {
                    "name": func.__name__,
                    "description": func.__doc__,
                    "parameters": {
                        "type": "object",
                        "properties": params
                    },
                    "required": required
                }
            })
        return tools 

    @property
    def saved_chats_dir(self) -> str:
        SAVED_CHATS_DIR = "saved_chats"
        current_dir = pathlib.Path(__file__).parent.resolve()
        directory = os.path.join(current_dir, SAVED_CHATS_DIR)
        if not os.path.exists(directory):
            os.makedirs(directory)
        return directory
    
    def reset_chat(self) -> None:
        self.messages = Messages()
        self.start_time = datetime.now()

    def delete_last_interaction(self) -> None:
        if self.messages.empty: return None
        self.messages.pop()
        last_message = self.messages[-1]
        while not last_message.displayed:
            if last_message.role == 'system' or self.messages.empty:
                break
            self.messages.pop()
            last_message = self.messages[-1]

    def _prepare_for_response(self, prompt: str, system_prompt: str) -> None:
        if self.messages.empty:
            self.messages.append("system", system_prompt)
        else:
            self.messages.edit(0, content=system_prompt)
        self.messages.append('user', prompt)      

    def get_stream_response(self, prompt: str, *, system_prompt: str, model: str, temperature: float) -> Generator[str, None, None]:        
        framework = get_model_framework(model)
        self._prepare_for_response(prompt=prompt, system_prompt=system_prompt)

        try:
            final_message = False
            while not final_message:
                final_message = True
                generator = self.clients[framework].get_streaming_response(model=model, temperature=temperature, messages=self.messages, tools=self.tools)
                for token in generator:
                    if isinstance(token, list):
                        # Only option is list of ToolCall
                        tool_calls: List[ToolCall] = token
                        final_message = False
                        if self.messages[self.messages.last_message_index].role == 'assistant':
                            self.messages.update(self.messages.last_message_index, tool_calls=tool_calls)
                        else:
                            self.messages.append('assistant', content=None, tool_calls=tool_calls, model=model)
                        for tool_call in tool_calls:
                            yield self.INFO_TOKEN + "Running tool: " + tool_call.function
                            self.logger.info(f"Running tool {tool_call.function}: {str(tool_call.arguments)}", color='yellow')
                            tool_output = self.callables[tool_call.function](**tool_call.arguments)
                            self.logger.debug("Tool output:\n" + tool_output)
                            self.messages.append(role='tool', content=tool_output, tool_calls=[tool_call])
                    else:
                        if self.messages[self.messages.last_message_index].role == 'assistant':
                            self.messages.update(self.messages.last_message_index, content=token)
                        else:
                            self.messages.append('assistant', content=token, model=model)
                        yield token + self.METADATA_TOKEN + model
        
        except Exception as e:
            self.logger.error(traceback.format_exc())
            yield f'âŒ _**{e.__class__.__name__}:** {e}_'

    def get_json_response(self, prompt: str, *, system_prompt: str, model: str, temperature: float) -> Generator[str, None, None]:
        framework = get_model_framework(model)
        self._prepare_for_response(prompt=prompt, system_prompt=system_prompt)

        final_message = False
        while not final_message:
            final_message = True
            response = self.clients[framework].get_json_response(model=model, temperature=temperature, messages=self.messages, tools=self.tools)
            for chunk in response:
                if isinstance(chunk, list):
                    # Only option is list of ToolCall
                    final_message = False
                    tool_calls: List[ToolCall] = chunk
                    self.messages.append(role='assistant', content=None, tool_calls=tool_calls, model=model)
                    for tool_call in tool_calls:
                        yield self.INFO_TOKEN + "Running tool: "  + tool_call.function
                        self.logger.info(f"Running tool {tool_call.function}: {str(tool_call.arguments)}", color='yellow')
                        tool_output = self.callables[tool_call.function](**tool_call.arguments)
                        self.logger.debug("Tool output:\n" + tool_output)
                        self.messages.append(role='tool', content=tool_output, tool_calls=[tool_call])
                else:
                    self.messages.append(role='assistant', content=chunk, model=model)
                    yield chunk + self.METADATA_TOKEN + model

    def export_chat(self) -> None:
        SavedChat(messages=self.messages, start_time=self.start_time, directory=self.saved_chats_dir).save_to_file()

    def load_chat(self, title: str, start_time: datetime) -> None:
        self.reset_chat()
        saved_chat = SavedChat.from_chat_title_and_time(title, start_time, directory=self.saved_chats_dir)
        self.messages = saved_chat.messages

    def get_displayed_messages(self) -> List[Message]:
        return [message for message in self.messages if message.displayed]
    
    def list_saved_chats(self) -> List[Tuple[str, datetime]]:
        chat_filenames = [f for f in os.listdir(self.saved_chats_dir) if os.path.isfile(os.path.join(self.saved_chats_dir, f)) and f.startswith(SavedChat.FILE_PREFIX) and f.endswith(SavedChat.FILE_SUFFIX)]
        saved_list = [SavedChat.filename_to_title_and_time(filename) for filename in chat_filenames]
        return sorted(saved_list, key=lambda t: t[1], reverse=True)
